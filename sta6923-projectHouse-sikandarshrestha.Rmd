The real estate markets, like those in Sydney and Melbourne, present an interesting opportunity for data analysts to analyze and predict where property prices are moving towards. Prediction of property prices is becoming increasingly important and beneficial. Property prices are a good indicator of both the overall market condition and the economic health of a country. Considering the data provided, we are wrangling a large set of property sales records stored in an unknown format and with unknown data quality issues

Data Source link: <https://www.kaggle.com/code/yaminimeena/house-price-prediction/input>

## 1. Extraction & Cleaning

```{r}
house<- read.csv("House Price Prediction Dataset.csv")
head(house)
```

```{r}
# structure function 
str(house)
```

```{r}
colSums(is.na(house))
```

**Missing values:** In this dataset, there is no missing values.

```{r}
# create new column and convert into factor
house['TotalRooms'] = house['Bedrooms'] + house['Bathrooms']
```

```{r}
str(house)
```

```{r}
# Function to convert `character` columns to `factor`, removed `Id` 
updated_df <- function(df) {
      
      # Identify columns that are categorical
      cat_cols <- c('Bedrooms', 'Bathrooms', 'Floors', 'Location', 'Condition', 'Garage','TotalRooms')
                                        
      # Convert categorical columns to factors
      df[cat_cols] <- lapply(df[cat_cols], as.factor)
                                        
      # Remove 'Id' column
      df <- df[, !names(df) %in% "Id"]
      # Return updated dataframe
      return(df)
}

```

```{r}
house <- updated_df(house)
```

```{r}
str(house)
```

```{r}
cols_fac <- names(house)[sapply(house, is.factor)]
cols_num <- names(house)[sapply(house, is.numeric)]
cat('cols_fac=>',cols_fac,'\n')
cat('cols_num=>',cols_num,'\n')
```

```{r}
library(e1071)
skewed <- apply(house[cols_num],2, skewness) # 2 indicates column
print("skewed of Original Units")
skewed
```

```{r}
library(caret)
# Apply BoxCox transformation to numeric predictors
housePP<- preProcess(house,method ="BoxCox")
```

```{r}
# Transform the `house` data
house_trans<- predict(housePP,house)
```

```{r}
str(house)
```

```{r}
str(house_trans)
```

```{r}
histogram(house$Price,
          xlab="natural units",
          type="count",
          main="original")
```

```{r}
histogram(house_trans$Price,
          xlab="log units",
          type="count",
          main="log-transformation")
```

**Interpretation:**

Since skewness is most likely lower, so there is not significant change even after the Box-Cox transformation. Hence we don't need to transformed the features.

## Binning on `YearBuilt` variable

```{r}
house$YearBuiltBin <- cut(
                    house$YearBuilt,
                    breaks = c(1900, 1925,1950, 1975, 2000, 2025),
                    labels = c("1900–1925", "1926–1950", "1951-1975", "1976–2000", "2001–2025"),
                    include.lowest = TRUE
                  )
```

```{r}
table(house$YearBuiltBin)
```

```{r}
str(house)
```

```{r}
# remove `YearBuilt` variabel from dataset `house`
house <- house[,-5]
```

```{r}
str(house)
```

## 2. Descriptive Statistics & Data Overview:-

```{r}
library(psych)
describe(house)
```

```{r}
summary(house)
```

## 2.1 Initial insights from Numerical data:

-   **Area:** Properties also show a large range in size (501 to 4,999 sq ft) with mean sq.ft is 2786.21.

-   **Price:** Property prices vary from \$50,005.00 to \~ \$1 million with average price is around \$537,676.85. Its SD is higher(\$276428.85), so it indicates significantly dispersion.

## 2.2 Initial insights from categorical data:

**Location=\>**

-   Downtown: 558 Rural: 474 Suburban: 483 Urban: 485

<!-- -->

-   The distribution is relatively balanced within rural, suburban, & urban. But, downtown has more properties.

**Condition=\>**

-   Excellent:511 Fair:521 Good:461 Poor:507

-   The distribution across condition grades are fairly unifrom except Good one.

**Garage=\>**

-   No:1038 Yes:962

-   There is almost an even split between properties having Garage or not.

**YearBuilt=\>**

-   The distribution is almost evenly uniform at each 25 years period from 1900 to 2025.

## 3. EDA (Exploration Data Analysis) / Data Visualizations

## 3.1 Distribution of all numeric predictors including target `Price` w/o transformation

```{r}
cols_fac <- names(house)[sapply(house, is.factor)]
cols_num <- names(house)[sapply(house, is.numeric)]
cat('cols_fac=>',cols_fac,'\n')
cat('cols_num=>',cols_num,'\n')
```

```{r}
options(scipen = 999)   # turn off scientific notation
# Histograms for all numerical predictors in house w/o transformation
for (col in cols_num) {
                        hist(house[[col]],
                        main = paste("Original Histogram of", col),
                        xlab = col,
                        col = "skyblue",
                        border = "white")
                                  }
                                  
```

## 3.2 Scatter Plotting for `Price` by num predictors `Area` using ggplot( Grammer of Graphics plot)

```{r}
cols_num
```

```{r}
plot(house$Area, house$Price,
     main = "Scatter Plot of Price vs Area",
     xlab = "Area",
     ylab = "Price",
     pch = 19,
     col = "blue")
abline(lm(Price ~ Area, data = house), col = "red", lwd = 2)

```

## 3.3 Box plot `Price` vs `categorical predictors` by using `For loop`

```{r}
for (col in cols_fac){
  boxplot <- ggplot(house, aes_string(x = col, y = 'Price')) +
             geom_boxplot(fill = "skyblue") +
             labs(title = paste("Price by", col))+
             theme_minimal()
        
  
  print(boxplot)
  
}
```

**Key Insight:** All predictors do not have outliers, so they are not making noises. All correlations are extremely weak (close to 0), with the strongest being `Floors-Price` at only 0.056. This suggests these features alone have very little linear relationship with house `Price` in your dataset.

**Potential Data Challenges**

Although the dataset is clean and well-structured, several limitations should be noted: ● Outliers in Area and Price: Although it does not have explicit NA’s, extremely high values are likely in these two variables. ● YearBuilt transformation: The range of this variable is from 1900–2023. If we don’t transform it, it may affect the model performance. ● Geographic bias: The variable Location only describes some neighborhoods (e.g.downtown, urban, etc.), this may not generalize all the house market. ● Heteroskedasticity: Higher-priced homes show bigger prediction errors, suggesting a log transformation of Price may be needed.

## (1) What are the most significant factors influencing house prices?

```{r}
str(house)
```

```{r}
model_lm_full <- lm(Price ~.,data=house)
```

```{r}
summary(model_lm_full)
```

Interpretation: At 5% significance level, the predictors `Bathrooms2` ,`Floor3`,`TotalRooms4`, `TotalRooms6` are the statisitically significant with the response variable `Price`. In other words, these are the influencing factors for the `house price`. Similarly, at 10% significance level, the predictors `ConditionFair`, `TotalRooms3`, `YearBuiltBin1951-1975` are the influencing factors for the `house price`. Since F-statistic p-value is 0.04985\<0.05, Overall the model is statistically significant. That means, at least one predictors are affects the response.

**Conclusion:**

In this model `model_lm_full` , the number of floors, bedrooms, & total rooms are the strong predictor of `Price` using a simple linear multiple regression model.Also, R2 =0.02002, implies the model explains only about 2% of price variation. This indicates that the relationship is not linear or some important predictors are missing (eg lot size, neighborhood rating, renovations e.t.c.)

## (2) Does the age of a house affect its market value?

```{r}
options(scipen=999) # turn off scientific notation
boxplot(Price~YearBuiltBin, data=house,
        xlab='Year Built',
        ylab='Price',
        main='Boxplot of House Price vs Year Built',
        col='lightblue')
```

[**Interpretation:**]{.underline}

The age of house is measured by `YearBuiltBin`.

From the summary of the linear multiple regression model `model_lm_full` , all the year built bin coefficients have high p-value \>0.05. So there are no significant effect on the response variable `Price`in the linear model. However, the effect appear on the nonlinear model and may interact with other predictors.

[**Conclusion:**]{.underline}

Although the `YearBuilt` does not significantly predict in this linear model, but

1)  we should test interaction effects between predictors.

2)  we may need a nonlinear model or log(Price) log transformation.

## (3) Do interactions between variables improve model accuracy?

```{r}
model_lm_interact <- lm(log(Price) ~ Area*Location + Condition*Location + Bedrooms*Bathrooms, data = house)
summary(model_lm_interact)

```

## Key summary statistics from output of the model `model_lm_interact` :

| Metrics             | value    | Interpretation                                                                                                  |
|---------------------|----------|-----------------------------------------------------------------------------------------------------------------|
| R2                  | 0.02376  | The model explains only about 2% of price variation.                                                            |
| Adjusted R2         | 0.004845 | After adjusting for the number of predictors, the model explains about 0.09% of price variation.                |
| F-statistic p-value | 0.1369   | Overall the model is not statistically significant. That means, the interactions does not affects the response. |

**Conclusion:**

1)  The extra interaction terms added complexity without improving predictive power of model. This means, interactions are only beneficial if there is evidence that one variable depends on another variable.

2)  The interactions ( like Area\**Location ,Condition\**Location , Bedrooms\*Bathrooms) do not meaningfully change the `Price`.

3)  Most of the interaction coefficients have high p-value \>0.05, so they are not statistically significant with the `Price`.

Hence due to all these reasons, i concluded that the model is overfitting without gaining accuracy.

## comparison between `Baseline linear multiple model` vs `interraction model`

+-------------------------------------------------------------+-------------+--------------------------------------------------------------------------------------+
| Model                                                       | Adjusted R2 | Explain                                                                              |
+=============================================================+=============+======================================================================================+
| Baseline simple multiple regression model (no interactions) | 0.003213    | \~0.3213% of variation of `Price` accounted in a model against the no. of variables. |
|                                                             |             |                                                                                      |
| ( `model_lm_full`)                                          |             |                                                                                      |
+-------------------------------------------------------------+-------------+--------------------------------------------------------------------------------------+
| Simple multiple regression model (with interactions)        | 0.0008615   | \~0.086155 of variation of `Price` .                                                 |
|                                                             |             |                                                                                      |
| (`model_lm_interact`)                                       |             |                                                                                      |
+-------------------------------------------------------------+-------------+--------------------------------------------------------------------------------------+

Conclusion:

From the comparison table, `model_lm_interact` model is not better as compare to `mode_lm_full` but it still does not explain the `Price` variation.

```{r}
# Convert factors to dummy variables
# `-1` drops intercept column
# house_dummies <- model.matrix(~., data = house) 
house_dummies <- model.matrix(~.-1, data = house) 
house_dummies <- as.data.frame(house_dummies)
```

```{r}
str(house_dummies)
```

```{r}
head(house_dummies)
```

```{r}
library(corrplot)
cor_matrix <- cor(house_dummies, use = "pairwise.complete.obs")

# plot
corrplot(cor_matrix,
         method = "color",
         addCoef.col = "black",
         number.cex = 0.45,
         tl.cex = 0.6,
         tl.col = "black",
         title = "Correlation Matrix with Dummy Variables",
         mar = c(0,0,1,0))
```

```{r}
str(house_dummies)
```

```{r}
model_lm_full <- lm(Price ~.,data=house_dummies)
```

```{r}
summary(model_lm_full)
```

```{r}
library(car)
vif(model_lm_full)
```

```{r}
```

```{r}

```
